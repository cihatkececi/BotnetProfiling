{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "\n",
    "Flow aggregation by a time window\n",
    "\n",
    "Generated features:\n",
    "* NumSrcPorts\n",
    "* NumDestAddr\n",
    "* NumDestPorts\n",
    "* NumFlows\n",
    "* NumBytesSum\n",
    "* NumBytesMean\n",
    "* NumBytesVar\n",
    "* NumPacketsSum\n",
    "* NumPacketsMean\n",
    "* NumPacketsVar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"NumSrcPorts\", \"NumDestAddr\", \"NumDestPorts\", \"NumFlows\",\n",
    "                   \"NumBytesSum\", \"NumBytesMean\", \"NumBytesVar\",\n",
    "                   \"NumPacketsSum\", \"NumPacketsMean\", \"NumPacketsVar\"]\n",
    "\n",
    "def calc_confusion_matrix(y_t, y_p, encoding):\n",
    "    \"\"\"Calculate Confusion matrix and count hits and misses\"\"\"\n",
    "    \n",
    "    confusion_matrix = np.zeros((13,13)).astype(int)\n",
    "\n",
    "    hit = 0\n",
    "    miss = 0\n",
    "    \n",
    "    for i in range(len(y_t)):\n",
    "        if encoding == 'ohe':\n",
    "            pred = y_p[i].argmax()\n",
    "            truth = y_t[i].argmax()\n",
    "        else:\n",
    "            pred = y_p[i]-1\n",
    "            truth = y_t[i]-1\n",
    "            \n",
    "        if pred == truth:\n",
    "            confusion_matrix[pred, pred] += 1\n",
    "            hit += 1\n",
    "        else:\n",
    "            confusion_matrix[truth, pred] += 1\n",
    "            miss += 1\n",
    "    return confusion_matrix, hit, miss\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    http://scikit-learn.org/stable/auto_examples/\n",
    "    model_selection/plot_confusion_matrix.html\n",
    "    \"\"\"\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    #print(cm)\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "def load_pickle_data(scenario = -1):\n",
    "    if scenario == -1:\n",
    "        p_filename = \"anomaly_cache2/anomaly_feat_list.p\"\n",
    "    else:\n",
    "        p_filename = \"anomaly_cache2/anomaly_scenario_features_\"+str(scenario)+\".p\"\n",
    "        \n",
    "    return pickle.load(open(p_filename, \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NumSrcPorts</th>\n",
       "      <th>NumDestAddr</th>\n",
       "      <th>NumDestPorts</th>\n",
       "      <th>NumFlows</th>\n",
       "      <th>NumBytesSum</th>\n",
       "      <th>NumBytesMean</th>\n",
       "      <th>NumBytesVar</th>\n",
       "      <th>NumPacketsSum</th>\n",
       "      <th>NumPacketsMean</th>\n",
       "      <th>NumPacketsVar</th>\n",
       "      <th>IsBotnet</th>\n",
       "      <th>Scenario</th>\n",
       "      <th>IsBackground</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>594.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>75.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1567.0</td>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>560.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>76.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   NumSrcPorts  NumDestAddr  NumDestPorts  NumFlows  NumBytesSum  \\\n",
       "0            1            1             1         1        594.0   \n",
       "1            1            1             1         1         75.0   \n",
       "2            1            1             1         1       1567.0   \n",
       "3            1            1             1         1        560.0   \n",
       "4            1            1             1         1         76.0   \n",
       "\n",
       "   NumBytesMean  NumBytesVar  NumPacketsSum  NumPacketsMean  NumPacketsVar  \\\n",
       "0         594.0          0.0           16.0            16.0            0.0   \n",
       "1          75.0          0.0            2.0             2.0            0.0   \n",
       "2        1567.0          0.0           10.0            10.0            0.0   \n",
       "3         560.0          0.0            4.0             4.0            0.0   \n",
       "4          76.0          0.0            2.0             2.0            0.0   \n",
       "\n",
       "  IsBotnet  Scenario IsBackground  \n",
       "0        0         1            1  \n",
       "1        0         1            1  \n",
       "2        0         1            1  \n",
       "3        0         1            1  \n",
       "4        0         1            1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_list = load_pickle_data()\n",
    "gen_feat_s_df = pd.concat(feat_list)\n",
    "gen_feat_s_df.reset_index(drop=True, inplace=True)\n",
    "gen_feat_s_df['IsBackground'] = 1-gen_feat_s_df['IsBotnet']\n",
    "gen_feat_s_df[features[:4]] = gen_feat_s_df[features[:4]].astype(int)\n",
    "gen_feat_s_df[features[4:]] = gen_feat_s_df[features[4:]].astype(float)\n",
    "\n",
    "gen_feat_s_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We Split train and test data 70% - 30%\n",
    "\n",
    "The reason that we have two different methods is: sklearn learning algorithms accept different kinds of inputs.\n",
    "\n",
    "We want to feed the algorithms with one-hot encoded outputs where each scenario is represented by a different feature and is eaither one or zero. But unfortunatelly, Logistic Regression and SVM classifiers only accepts Integer Valued Encoding which creates a bias between classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate test and train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integer Value Encoding for LR and SVM\n",
    "def generate_train_test_ive():\n",
    "    # Generate test and train sets\n",
    "    #aa = pd.concat([gen_feat_s_df[gen_feat_s_df['IsBackground'] == 1].sample(10000), gen_feat_s_df[gen_feat_s_df['IsBackground'] == 0]], axis=0).reset_index(drop=True)\n",
    "    \n",
    "    X = gen_feat_s_df[features].values\n",
    "    y = gen_feat_s_df['IsBackground'].values\n",
    "    y = y.astype(int)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state=25)\n",
    "\n",
    "    # Feature Counts\n",
    "    print(\"Feature Counts\\n\")\n",
    "    print(\"num\\ttrain\\ttest\\ttotal\")\n",
    "    print(\"-\" * 30)\n",
    "    for i in range(2):\n",
    "        print(\"{0}\\t{1}\\t{2}\\t{3}\".format(i, np.count_nonzero(y_train==i), np.count_nonzero(y_test==i), np.count_nonzero(y==i)))\n",
    "    print(\"-\" * 30)\n",
    "    print(\"total:\\t{0}\\t{1}\\t{2}\".format(len(y_train), len(y_test), len(y)))\n",
    "\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    print()\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all, test on all\n",
    "# One-Hot Encoding for KNN and RF\n",
    "def generate_train_test_ohe():\n",
    "    # Generate test and train sets\n",
    "    \n",
    "    X = gen_feat_s_df[features].values\n",
    "    y = gen_feat_s_df[['IsBackground','IsBotnet']].values\n",
    "    y = y.astype(int)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state=25)\n",
    "\n",
    "\n",
    "    # Scale Data\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "    # Feature Counts\n",
    "    print(\"Feature Counts\\n\")\n",
    "    print(\"num\\ttrain\\ttest\\ttotal\")\n",
    "    print(\"-\" * 30)\n",
    "    for i in range(2):\n",
    "        print(\"{0}\\t{1}\\t{2}\\t{3}\".format(i, np.count_nonzero(y_train[:,i]), np.count_nonzero(y_test[:,i]), np.count_nonzero(y[:,i])))\n",
    "    print(\"-\" * 30)\n",
    "    print(\"total:\\t{0}\\t{1}\\t{2}\".format(len(y_train), len(y_test), len(y)))\n",
    "    print()\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Train: 3 4 5 7 10 11 12 13\n",
    "#* Test:  1 2 6 8 9\n",
    "def generate_train_test_ohe2():\n",
    "    # Generate test and train sets\n",
    "    x_tr = gen_feat_s_df[(gen_feat_s_df['Scenario'] == 3) | (gen_feat_s_df['Scenario'] == 4)\n",
    "                     | (gen_feat_s_df['Scenario'] == 5) | (gen_feat_s_df['Scenario'] == 7)\n",
    "                     | (gen_feat_s_df['Scenario'] == 10) | (gen_feat_s_df['Scenario'] == 11)\n",
    "                     | (gen_feat_s_df['Scenario'] == 12) | (gen_feat_s_df['Scenario'] == 13)]\n",
    "\n",
    "    X_ts = []\n",
    "    y_ts = []\n",
    "    \n",
    "    X = gen_feat_s_df[features].values\n",
    "    y = gen_feat_s_df[['IsBackground','IsBotnet']].values\n",
    "    y = y.astype(int)\n",
    "\n",
    "    X_train, y_train = x_tr[features].values, x_tr[['IsBackground','IsBotnet']].values\n",
    "    y_train= y_train.astype(int)\n",
    "    \n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    \n",
    "    for i in [1,2,6,8,9]:\n",
    "        X_test = gen_feat_s_df[gen_feat_s_df['Scenario'] == i][features].values\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        y_test = gen_feat_s_df[gen_feat_s_df['Scenario'] == i][['IsBackground','IsBotnet']].values\n",
    "        y_test = y_test.astype(int)\n",
    "        \n",
    "        X_ts.append(X_test)\n",
    "        y_ts.append(y_test)\n",
    "    \n",
    "    return X_train, X_ts, y_train, y_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train - Test on each Scenario Seperately\n",
    "def generate_train_test_ohe3():\n",
    "    # Generate test and train sets\n",
    "    X_tr = []\n",
    "    y_tr = []\n",
    "    \n",
    "    X_ts = []\n",
    "    y_ts = []\n",
    "    \n",
    "    for i in range(1,14):\n",
    "        X = gen_feat_s_df[gen_feat_s_df['Scenario'] == i][features].values\n",
    "        y = gen_feat_s_df[gen_feat_s_df['Scenario'] == i][['IsBackground','IsBotnet']].values\n",
    "        y = y.astype(int)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state=25)\n",
    "        \n",
    "        scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "    \n",
    "        X_tr.append(X_train)\n",
    "        y_tr.append(y_train)\n",
    "        \n",
    "        X_ts.append(X_test)\n",
    "        y_ts.append(y_test)\n",
    "    \n",
    "    return X_tr, X_ts, y_tr, y_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions - Prints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_all_results(y_test, y_pred, encoding):\n",
    "    if encoding == 'ohe':\n",
    "        y_t1 = y_test[:,0]\n",
    "        y_p1 = y_pred[:,0]\n",
    "    else:\n",
    "        y_t1 = y_test.copy()\n",
    "        y_p1 = y_pred.copy()\n",
    "        \n",
    "    print(\"Anomaly Detection:\")\n",
    "    print(\"\\nClassification report:\")\n",
    "    print(metrics.classification_report(y_t1, y_p1, target_names=[\"Botnet\", \"Background\"]))\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(metrics.confusion_matrix(y_t1, y_p1))\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Models - (Cross Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_cv(C=1e5, cv=5):\n",
    "    log_regression = LogisticRegression(C=C)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = generate_train_test_ive()\n",
    "\n",
    "    scores = cross_val_score(log_regression, X_train, y_train, cv=cv)\n",
    "    print(\"CV Score: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "    \n",
    "def svm_classifier_cv(kernel='rbf', C=1e5, cv=5):\n",
    "    svm_clf = svm.SVC(kernel=kernel, C=C)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = generate_train_test_ive()\n",
    "\n",
    "    scores = cross_val_score(svm_clf, X_train, y_train, cv=cv)\n",
    "    print(\"CV Score: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "    \n",
    "def knn_classifier_cv(cv=5):\n",
    "    cv_scores = []\n",
    "    k_vals = []\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = generate_train_test_ohe()\n",
    "    for i in range(10):\n",
    "        k = 2*i+1\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        \n",
    "        scores = cross_val_score(knn, X_train, y_train, cv=cv)\n",
    "        print(\"CV Score (k = %d): %0.2f (+/- %0.2f)\" % (k, scores.mean(), scores.std() * 2))\n",
    "        \n",
    "        cv_scores.append(scores.mean())\n",
    "        k_vals.append(k)\n",
    "        \n",
    "    # print_results()\n",
    "    plt.plot(k_vals, cv_scores)\n",
    "    plt.title(\"CV Scores vs. k values\")\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(\"CV Score\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "def random_forest_cv(n_estimators=25, cv=5):\n",
    "    rand_forest_clf = RandomForestClassifier(n_estimators=n_estimators)\n",
    "\n",
    "    X_train, X_test, y_train, y_test= generate_train_test_ohe()\n",
    "\n",
    "    scores = cross_val_score(rand_forest_clf, X_train, y_train, cv=cv)\n",
    "    print(\"CV Score: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Counts\n",
      "\n",
      "num\ttrain\ttest\ttotal\n",
      "------------------------------\n",
      "0\t417107\t178832\t595939\n",
      "1\t3931\t1613\t5544\n",
      "------------------------------\n",
      "total:\t421038\t180445\t601483\n",
      "\n",
      "CV Score: 1.00 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "random_forest_cv(1, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models - (Train -Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(C=1e5):\n",
    "    log_regression = LogisticRegression(C=C)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = generate_train_test_ive()\n",
    "\n",
    "    # Train the classifier\n",
    "    log_regression.fit(X_train, y_train)\n",
    "\n",
    "    # Predict\n",
    "    y_pred = log_regression.predict(X_test)\n",
    "\n",
    "    print()\n",
    "    print('*'*35)\n",
    "    print_all_results(y_test, y_pred, 'ive')\n",
    "    \n",
    "def svm_classifier(kernel='rbf', C=5):\n",
    "    svm_clf = svm.SVC(kernel=kernel, C=C)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = generate_train_test_ive()\n",
    "\n",
    "    svm_clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = svm_clf.predict(X_test)\n",
    "\n",
    "    print()\n",
    "    print('*'*35)\n",
    "    print_all_results(y_test, y_pred, 'ive')\n",
    "    \n",
    "def knn_classifier(n_neighbors, tt_set = 1):\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    \n",
    "    if tt_set == 1:\n",
    "        X_train, X_test, y_train, y_test = generate_train_test_ohe()\n",
    "\n",
    "        knn.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = knn.predict(X_test)\n",
    "\n",
    "        print()\n",
    "        print('*'*35)\n",
    "        print_all_results(y_test, y_pred, 'ohe')\n",
    "        \n",
    "    elif tt_set == 2:\n",
    "        X_train, X_test, y_train, y_test= generate_train_test_ohe2()\n",
    "\n",
    "        knn.fit(X_train, y_train)\n",
    "        \n",
    "        test_order = [1,2,6,8,9]\n",
    "        for i in range(len(X_test)):\n",
    "            y_pred = knn.predict(X_test[i])\n",
    "            \n",
    "            print(\"Scenario \" + str(test_order[i]))\n",
    "            print_all_results(y_test[i], y_pred, 'ohe')\n",
    "            print()\n",
    "            print('*'*35)\n",
    "            print()\n",
    "            \n",
    "    elif tt_set == 3:\n",
    "        X_train, X_test, y_train, y_test= generate_train_test_ohe3()\n",
    "\n",
    "        for i in range(len(X_train)):\n",
    "            knn.fit(X_train[i], y_train[i])\n",
    "            y_pred = knn.predict(X_test[i])\n",
    "            \n",
    "            print(\"Scenario \" + str(i+1))\n",
    "            print_all_results(y_test[i], y_pred, 'ohe')\n",
    "            print()\n",
    "            print('*'*35)\n",
    "            print()\n",
    "    \n",
    "def random_forest(n_estimators=5, graph = False, tt_set = 1):\n",
    "    rand_forest_clf = RandomForestClassifier(n_estimators=n_estimators)\n",
    "\n",
    "    if tt_set == 1:\n",
    "        X_train, X_test, y_train, y_test= generate_train_test_ohe()\n",
    "\n",
    "        rand_forest_clf.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = rand_forest_clf.predict(X_test)\n",
    "        \n",
    "        print()\n",
    "        print('*'*35)\n",
    "        print_all_results(y_test, y_pred, 'ohe')\n",
    "        \n",
    "\n",
    "        importances = np.round(rand_forest_clf.feature_importances_, 3)\n",
    "        print(pd.DataFrame(np.array([features, importances]).T, columns=['Features', 'Importance']))\n",
    "\n",
    "\n",
    "        if graph:\n",
    "            #https://stats.stackexchange.com/questions/130206/sklearn-tree-export-graphviz-values-do-not-add-up-to-samples\n",
    "            from sklearn.tree import export_graphviz\n",
    "\n",
    "            class_names = [str(i) for i in rand_forest_clf.classes_]\n",
    "\n",
    "            export_graphviz(rand_forest_clf.estimators_[0],\n",
    "                            feature_names=features,\n",
    "                            class_names=class_names,\n",
    "                            filled=True,\n",
    "                            rounded=True)\n",
    "\n",
    "            os.system('dot -Tpng tree.dot -o tree.png')\n",
    "        \n",
    "    elif tt_set == 2:\n",
    "        X_train, X_test, y_train, y_test= generate_train_test_ohe2()\n",
    "\n",
    "        rand_forest_clf.fit(X_train, y_train)\n",
    "        \n",
    "        test_order = [1,2,6,8,9]\n",
    "        for i in range(len(X_test)):\n",
    "            y_pred = rand_forest_clf.predict(X_test[i])\n",
    "            \n",
    "            print(\"Scenario \" + str(test_order[i]))\n",
    "            print_all_results(y_test[i], y_pred, 'ohe')\n",
    "            print()\n",
    "            print('*'*35)\n",
    "            print()\n",
    "            \n",
    "    elif tt_set == 3:\n",
    "        X_train, X_test, y_train, y_test= generate_train_test_ohe3()\n",
    "\n",
    "        for i in range(len(X_train)):\n",
    "            rand_forest_clf.fit(X_train[i], y_train[i])\n",
    "            y_pred = rand_forest_clf.predict(X_test[i])\n",
    "            \n",
    "            print(\"Scenario \" + str(i+1))\n",
    "            print_all_results(y_test[i], y_pred, 'ohe')\n",
    "            print()\n",
    "            print('*'*35)\n",
    "            print()\n",
    "            \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Counts\n",
      "\n",
      "num\ttrain\ttest\ttotal\n",
      "------------------------------\n",
      "0\t417107\t178832\t595939\n",
      "1\t3931\t1613\t5544\n",
      "------------------------------\n",
      "total:\t421038\t180445\t601483\n",
      "\n",
      "\n",
      "***********************************\n",
      "Anomaly Detection:\n",
      "\n",
      "Classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     Botnet       0.92      0.91      0.91      1613\n",
      " Background       1.00      1.00      1.00    178832\n",
      "\n",
      "avg / total       1.00      1.00      1.00    180445\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  1464    149]\n",
      " [   124 178708]]\n",
      "\n",
      "         Features Importance\n",
      "0     NumSrcPorts      0.312\n",
      "1     NumDestAddr      0.074\n",
      "2    NumDestPorts      0.104\n",
      "3        NumFlows      0.019\n",
      "4     NumBytesSum      0.066\n",
      "5    NumBytesMean      0.137\n",
      "6     NumBytesVar      0.002\n",
      "7   NumPacketsSum      0.191\n",
      "8  NumPacketsMean       0.09\n",
      "9   NumPacketsVar      0.004\n"
     ]
    }
   ],
   "source": [
    "random_forest(n_estimators=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
